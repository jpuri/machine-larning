{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1800,
   "id": "06073e9a-5436-4e7f-af32-0deaffe901c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1801,
   "id": "726c81dc-7ea9-42bf-a824-c8321d247e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from loan approval csv into python lists\n",
    "\n",
    "data=pd.read_csv(\"loan_approval_dataset.csv\")\n",
    "\n",
    "# check the columns in csv\n",
    "data.columns = data.columns.str.strip()\n",
    "# print(data.columns)\n",
    "\n",
    "X_original = data[[\n",
    "    'no_of_dependents',\n",
    "    'education',\n",
    "    'self_employed',\n",
    "    'income_annum',\n",
    "    'loan_amount',\n",
    "    'loan_term',\n",
    "    'cibil_score',\n",
    "    'residential_assets_value',\n",
    "    'commercial_assets_value',\n",
    "    'luxury_assets_value',\n",
    "    'bank_asset_value',\n",
    "]].values.tolist()\n",
    "\n",
    "X = X_original\n",
    "\n",
    "# code below normalises data\n",
    "X = []\n",
    "for row in X_original:\n",
    "    row[1] = 10 if row[1].strip().lower() == 'graduate' else 0\n",
    "    row[2] = 10 if row[2].strip().lower() == 'yes' else 0\n",
    "    row_new = []\n",
    "    for i in range(0, len(row)):\n",
    "        if (i < 1 or i > 2):\n",
    "            row_new.append(row[i] / X_original[0][i] * 10)\n",
    "        else:\n",
    "            row_new.append(row[i])\n",
    "    X.append(row_new)\n",
    "\n",
    "# reading loan approval status into Y as 1 or 0\n",
    "Y = data[\"loan_status\"].values.tolist()\n",
    "Y = list(map(lambda st: 1 if st.strip() == 'Approved' else 0, Y))\n",
    "\n",
    "# make X, Y numpy arrays for vectorisation\n",
    "X = np.array(X)\n",
    "X = X.reshape(11, X.shape[0])\n",
    "\n",
    "Y = np.array(Y)\n",
    "Y = Y.reshape(1, Y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1802,
   "id": "b6a2a1a4-93a1-48dc-b2d5-14c4baf055a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining sigmoid function\n",
    "\n",
    "def sigmoid(Z):\n",
    "\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1803,
   "id": "039bc3eb-7833-4364-955f-6f1d2e5a6c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid backward function\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "\n",
    "    Z = cache\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1 - s)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1804,
   "id": "585f14e4-6c55-416c-bdac-b0b027ec664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining relu function\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "\n",
    "    assert(A.shape == Z.shape)\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1805,
   "id": "418e7c53-2543-42ea-a4be-4a3b4b55270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid backward function\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "\n",
    "    for i in range(len(dZ)):\n",
    "        for j in range(len(dZ[i])):\n",
    "            dZ[i][j]  = max(Z[i][j], 0)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1806,
   "id": "4b94e04b-0763-45c5-a4c9-a79349d14acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising the parameters\n",
    "\n",
    "def initialise_parameters_deep(layer_dims):\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for i in range(1, L):\n",
    "        parameters['W' + str(i)] = np.random.rand(layer_dims[i], layer_dims[i - 1]) * .001\n",
    "        parameters['b' + str(i)] = np.zeros((layer_dims[i], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1807,
   "id": "dc28649e-0da2-452a-b784-65338d508859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear forward function\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "\n",
    "    return Z, cache "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1808,
   "id": "9b4af2c1-39c3-4440-a791-577c45dae2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear activation forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == 'sigmoid':\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == 'relu':\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1809,
   "id": "ed850572-308a-4d37-8766-223745b86e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete forward pass\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    for i in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(i)], parameters['b' + str(i)], activation='relu' )\n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation='sigmoid' )\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1810,
   "id": "861b4a64-e4ad-4d12-8543-250a733056f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    cost = (-1/m) * np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1811,
   "id": "49ca3f2c-8008-414b-9b6e-8dea7c40ccf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1812,
   "id": "411c5718-c8fe-4da9-9775-d4249852775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear activation backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation == 'sigmoid':\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    elif activation == 'relu':\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1813,
   "id": "fb0d67cd-82c7-4be5-a381-5e60f146d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete forward pass\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "    current_cache = caches[L - 1]\n",
    "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, activation = 'sigmoid')\n",
    "    grads['dA' + str(L - 1)] = dA_prev_temp\n",
    "    grads['dW' + str(L)] = dW_temp\n",
    "    grads['db' + str(L)] = db_temp\n",
    "\n",
    "    for i in reversed(range(L - 1)):\n",
    "        current_cache = caches[i]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads['dA' + str(i + 1)], current_cache, activation = 'relu')\n",
    "        grads['dA' + str(i)] = dA_prev_temp\n",
    "        grads['dW' + str(i + 1)] = dW_temp\n",
    "        grads['db' + str(i + 1)] = db_temp\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1814,
   "id": "b5a083af-37d4-460c-9385-db7ff95f376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update parameters\n",
    "\n",
    "def update_parameters(params, grads, size, learning_rate):\n",
    "    parameters = copy.deepcopy(params)\n",
    "    L = size\n",
    "\n",
    "    for i in range(L):\n",
    "        parameters['W' + str(i + 1)] = parameters['W' + str(i + 1)] - learning_rate * grads['dW' + str(i + 1)]\n",
    "        parameters['b' + str(i + 1)] = parameters['b' + str(i + 1)] - learning_rate * grads['db' + str(i + 1)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1815,
   "id": "df31598b-2677-4bc4-aa07-962e3751b486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep neural network model\n",
    "\n",
    "def L_layer_model(X, Y, layer_dims):\n",
    "\n",
    "    learning_rate = 0.0075\n",
    "    num_iterations = 300\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "\n",
    "    parameters = initialise_parameters_deep(layer_dims)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "\n",
    "        cost = compute_cost(AL, Y)\n",
    "        costs.append(cost)\n",
    "\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "\n",
    "        parameters = update_parameters(parameters, grads, len(layer_dims) - 1, learning_rate)\n",
    "\n",
    "        print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "\n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1816,
   "id": "6549222e-9f98-4188-82f2-f613922a0029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6931471805441671\n",
      "Cost after iteration 1: 0.6930353629417604\n",
      "Cost after iteration 2: 0.6929239642464395\n",
      "Cost after iteration 3: 0.6928129829043067\n",
      "Cost after iteration 4: 0.6927024173513647\n",
      "Cost after iteration 5: 0.6925922660293087\n",
      "Cost after iteration 6: 0.6924825273855058\n",
      "Cost after iteration 7: 0.6923731998729772\n",
      "Cost after iteration 8: 0.6922642819503804\n",
      "Cost after iteration 9: 0.69215577208199\n",
      "Cost after iteration 10: 0.69204766873768\n",
      "Cost after iteration 11: 0.6919399703929051\n",
      "Cost after iteration 12: 0.6918326755286828\n",
      "Cost after iteration 13: 0.6917257826315755\n",
      "Cost after iteration 14: 0.6916192901936714\n",
      "Cost after iteration 15: 0.6915131967125673\n",
      "Cost after iteration 16: 0.6914075006913496\n",
      "Cost after iteration 17: 0.6913022006385774\n",
      "Cost after iteration 18: 0.6911972950682639\n",
      "Cost after iteration 19: 0.6910927824998582\n",
      "Cost after iteration 20: 0.6909886614582278\n",
      "Cost after iteration 21: 0.6908849304736403\n",
      "Cost after iteration 22: 0.6907815880817458\n",
      "Cost after iteration 23: 0.6906786328235598\n",
      "Cost after iteration 24: 0.6905760632454435\n",
      "Cost after iteration 25: 0.6904738778990881\n",
      "Cost after iteration 26: 0.6903720753414967\n",
      "Cost after iteration 27: 0.6902706541349651\n",
      "Cost after iteration 28: 0.6901696128470659\n",
      "Cost after iteration 29: 0.6900689500506307\n",
      "Cost after iteration 30: 0.6899686643237317\n",
      "Cost after iteration 31: 0.6898687542496648\n",
      "Cost after iteration 32: 0.6897692184169322\n",
      "Cost after iteration 33: 0.689670055419225\n",
      "Cost after iteration 34: 0.6895712638554051\n",
      "Cost after iteration 35: 0.6894728423294894\n",
      "Cost after iteration 36: 0.6893747894506307\n",
      "Cost after iteration 37: 0.6892771038331016\n",
      "Cost after iteration 38: 0.6891797840962771\n",
      "Cost after iteration 39: 0.689082828864617\n",
      "Cost after iteration 40: 0.6889862367676494\n",
      "Cost after iteration 41: 0.6888900064399531\n",
      "Cost after iteration 42: 0.6887941365211407\n",
      "Cost after iteration 43: 0.6886986256558421\n",
      "Cost after iteration 44: 0.6886034724936861\n",
      "Cost after iteration 45: 0.6885086756892855\n",
      "Cost after iteration 46: 0.688414233902218\n",
      "Cost after iteration 47: 0.688320145797012\n",
      "Cost after iteration 48: 0.6882264100431265\n",
      "Cost after iteration 49: 0.6881330253149374\n",
      "Cost after iteration 50: 0.6880399902917194\n",
      "Cost after iteration 51: 0.6879473036576286\n",
      "Cost after iteration 52: 0.6878549641016872\n",
      "Cost after iteration 53: 0.6877629703177668\n",
      "Cost after iteration 54: 0.6876713210045705\n",
      "Cost after iteration 55: 0.6875800148656179\n",
      "Cost after iteration 56: 0.6874890506092283\n",
      "Cost after iteration 57: 0.6873984269485027\n",
      "Cost after iteration 58: 0.6873081426013106\n",
      "Cost after iteration 59: 0.6872181962902703\n",
      "Cost after iteration 60: 0.6871285867427341\n",
      "Cost after iteration 61: 0.6870393126907727\n",
      "Cost after iteration 62: 0.6869503728711579\n",
      "Cost after iteration 63: 0.6868617660253468\n",
      "Cost after iteration 64: 0.6867734908994654\n",
      "Cost after iteration 65: 0.6866855462442932\n",
      "Cost after iteration 66: 0.6865979308152463\n",
      "Cost after iteration 67: 0.6865106433723617\n",
      "Cost after iteration 68: 0.6864236826802816\n",
      "Cost after iteration 69: 0.6863370475082372\n",
      "Cost after iteration 70: 0.6862507366300329\n",
      "Cost after iteration 71: 0.6861647488240303\n",
      "Cost after iteration 72: 0.686079082873133\n",
      "Cost after iteration 73: 0.6859937375647692\n",
      "Cost after iteration 74: 0.6859087116908785\n",
      "Cost after iteration 75: 0.6858240040478941\n",
      "Cost after iteration 76: 0.6857396134367282\n",
      "Cost after iteration 77: 0.6856555386627559\n",
      "Cost after iteration 78: 0.6855717785357995\n",
      "Cost after iteration 79: 0.6854883318701146\n",
      "Cost after iteration 80: 0.6854051974843722\n",
      "Cost after iteration 81: 0.6853223742016452\n",
      "Cost after iteration 82: 0.6852398608493917\n",
      "Cost after iteration 83: 0.6851576562594409\n",
      "Cost after iteration 84: 0.6850757592679767\n",
      "Cost after iteration 85: 0.6849941687155232\n",
      "Cost after iteration 86: 0.6849128834469286\n",
      "Cost after iteration 87: 0.6848319023113516\n",
      "Cost after iteration 88: 0.6847512241622443\n",
      "Cost after iteration 89: 0.6846708478573389\n",
      "Cost after iteration 90: 0.6845907722586314\n",
      "Cost after iteration 91: 0.6845109962323671\n",
      "Cost after iteration 92: 0.6844315186490257\n",
      "Cost after iteration 93: 0.6843523383833066\n",
      "Cost after iteration 94: 0.6842734543141127\n",
      "Cost after iteration 95: 0.6841948653245377\n",
      "Cost after iteration 96: 0.6841165703018497\n",
      "Cost after iteration 97: 0.684038568137477\n",
      "Cost after iteration 98: 0.683960857726993\n",
      "Cost after iteration 99: 0.6838834379701024\n",
      "Cost after iteration 100: 0.6838063077706259\n",
      "Cost after iteration 101: 0.6837294660364857\n",
      "Cost after iteration 102: 0.6836529116796904\n",
      "Cost after iteration 103: 0.6835766436163225\n",
      "Cost after iteration 104: 0.6835006607665213\n",
      "Cost after iteration 105: 0.6834249620544702\n",
      "Cost after iteration 106: 0.6833495464083824\n",
      "Cost after iteration 107: 0.6832744127604854\n",
      "Cost after iteration 108: 0.6831995600470073\n",
      "Cost after iteration 109: 0.6831249872081635\n",
      "Cost after iteration 110: 0.6830506931881407\n",
      "Cost after iteration 111: 0.6829766769350842\n",
      "Cost after iteration 112: 0.6829029374010828\n",
      "Cost after iteration 113: 0.6828294735421555\n",
      "Cost after iteration 114: 0.6827562843182376\n",
      "Cost after iteration 115: 0.6826833686931651\n",
      "Cost after iteration 116: 0.682610725634663\n",
      "Cost after iteration 117: 0.6825383541143295\n",
      "Cost after iteration 118: 0.6824662531076237\n",
      "Cost after iteration 119: 0.6823944215938499\n",
      "Cost after iteration 120: 0.6823228585561462\n",
      "Cost after iteration 121: 0.6822515629814685\n",
      "Cost after iteration 122: 0.6821805338605778\n",
      "Cost after iteration 123: 0.6821097701880273\n",
      "Cost after iteration 124: 0.682039270962147\n",
      "Cost after iteration 125: 0.6819690351850318\n",
      "Cost after iteration 126: 0.6818990618625266\n",
      "Cost after iteration 127: 0.681829350004214\n",
      "Cost after iteration 128: 0.6817598986234006\n",
      "Cost after iteration 129: 0.6816907067371021\n",
      "Cost after iteration 130: 0.6816217733660324\n",
      "Cost after iteration 131: 0.6815530975345883\n",
      "Cost after iteration 132: 0.6814846782708369\n",
      "Cost after iteration 133: 0.6814165146065025\n",
      "Cost after iteration 134: 0.6813486055769536\n",
      "Cost after iteration 135: 0.6812809502211885\n",
      "Cost after iteration 136: 0.6812135475818241\n",
      "Cost after iteration 137: 0.6811463967050806\n",
      "Cost after iteration 138: 0.6810794966407705\n",
      "Cost after iteration 139: 0.6810128464422842\n",
      "Cost after iteration 140: 0.6809464451665782\n",
      "Cost after iteration 141: 0.6808802918741607\n",
      "Cost after iteration 142: 0.6808143856290796\n",
      "Cost after iteration 143: 0.6807487254989104\n",
      "Cost after iteration 144: 0.6806833105547416\n",
      "Cost after iteration 145: 0.6806181398711636\n",
      "Cost after iteration 146: 0.6805532125262549\n",
      "Cost after iteration 147: 0.6804885276015702\n",
      "Cost after iteration 148: 0.6804240841821266\n",
      "Cost after iteration 149: 0.6803598813563927\n",
      "Cost after iteration 150: 0.6802959182162746\n",
      "Cost after iteration 151: 0.6802321938571037\n",
      "Cost after iteration 152: 0.6801687073776248\n",
      "Cost after iteration 153: 0.6801054578799831\n",
      "Cost after iteration 154: 0.6800424444697118\n",
      "Cost after iteration 155: 0.6799796662557201\n",
      "Cost after iteration 156: 0.6799171223502803\n",
      "Cost after iteration 157: 0.6798548118690166\n",
      "Cost after iteration 158: 0.679792733930891\n",
      "Cost after iteration 159: 0.6797308876581932\n",
      "Cost after iteration 160: 0.679669272176527\n",
      "Cost after iteration 161: 0.6796078866147987\n",
      "Cost after iteration 162: 0.6795467301052049\n",
      "Cost after iteration 163: 0.6794858017832205\n",
      "Cost after iteration 164: 0.6794251007875867\n",
      "Cost after iteration 165: 0.6793646262602986\n",
      "Cost after iteration 166: 0.6793043773465944\n",
      "Cost after iteration 167: 0.6792443531949423\n",
      "Cost after iteration 168: 0.6791845529570291\n",
      "Cost after iteration 169: 0.6791249757877484\n",
      "Cost after iteration 170: 0.6790656208451888\n",
      "Cost after iteration 171: 0.6790064872906225\n",
      "Cost after iteration 172: 0.6789475742884927\n",
      "Cost after iteration 173: 0.678888881006403\n",
      "Cost after iteration 174: 0.6788304066151051\n",
      "Cost after iteration 175: 0.6787721502884869\n",
      "Cost after iteration 176: 0.6787141112035623\n",
      "Cost after iteration 177: 0.6786562885404586\n",
      "Cost after iteration 178: 0.6785986814824047\n",
      "Cost after iteration 179: 0.6785412892157205\n",
      "Cost after iteration 180: 0.6784841109298058\n",
      "Cost after iteration 181: 0.6784271458171276\n",
      "Cost after iteration 182: 0.6783703930732097\n",
      "Cost after iteration 183: 0.6783138518966214\n",
      "Cost after iteration 184: 0.6782575214889661\n",
      "Cost after iteration 185: 0.6782014010548704\n",
      "Cost after iteration 186: 0.6781454898019718\n",
      "Cost after iteration 187: 0.6780897869409088\n",
      "Cost after iteration 188: 0.6780342916853102\n",
      "Cost after iteration 189: 0.6779790032517815\n",
      "Cost after iteration 190: 0.6779239208598971\n",
      "Cost after iteration 191: 0.6778690437321876\n",
      "Cost after iteration 192: 0.6778143710941283\n",
      "Cost after iteration 193: 0.6777599021741295\n",
      "Cost after iteration 194: 0.6777056362035255\n",
      "Cost after iteration 195: 0.6776515724165628\n",
      "Cost after iteration 196: 0.6775977100503906\n",
      "Cost after iteration 197: 0.6775440483450487\n",
      "Cost after iteration 198: 0.6774905865434581\n",
      "Cost after iteration 199: 0.6774373238914085\n",
      "Cost after iteration 200: 0.6773842596375503\n",
      "Cost after iteration 201: 0.6773313930333817\n",
      "Cost after iteration 202: 0.6772787233332386\n",
      "Cost after iteration 203: 0.6772262497942844\n",
      "Cost after iteration 204: 0.6771739716765002\n",
      "Cost after iteration 205: 0.6771218882426723\n",
      "Cost after iteration 206: 0.6770699987583838\n",
      "Cost after iteration 207: 0.6770183024920035\n",
      "Cost after iteration 208: 0.6769667987146744\n",
      "Cost after iteration 209: 0.6769154867003055\n",
      "Cost after iteration 210: 0.6768643657255597\n",
      "Cost after iteration 211: 0.6768134350698444\n",
      "Cost after iteration 212: 0.6767626940153011\n",
      "Cost after iteration 213: 0.6767121418467951\n",
      "Cost after iteration 214: 0.6766617778519052\n",
      "Cost after iteration 215: 0.6766116013209141\n",
      "Cost after iteration 216: 0.6765616115467982\n",
      "Cost after iteration 217: 0.6765118078252162\n",
      "Cost after iteration 218: 0.6764621894545019\n",
      "Cost after iteration 219: 0.6764127557356509\n",
      "Cost after iteration 220: 0.6763635059723134\n",
      "Cost after iteration 221: 0.6763144394707825\n",
      "Cost after iteration 222: 0.6762655555399855\n",
      "Cost after iteration 223: 0.6762168534914728\n",
      "Cost after iteration 224: 0.6761683326394095\n",
      "Cost after iteration 225: 0.6761199923005645\n",
      "Cost after iteration 226: 0.6760718317943015\n",
      "Cost after iteration 227: 0.6760238504425681\n",
      "Cost after iteration 228: 0.6759760475698884\n",
      "Cost after iteration 229: 0.6759284225033508\n",
      "Cost after iteration 230: 0.6758809745726\n",
      "Cost after iteration 231: 0.6758337031098264\n",
      "Cost after iteration 232: 0.6757866074497579\n",
      "Cost after iteration 233: 0.6757396869296489\n",
      "Cost after iteration 234: 0.6756929408892719\n",
      "Cost after iteration 235: 0.6756463686709072\n",
      "Cost after iteration 236: 0.6755999696193344\n",
      "Cost after iteration 237: 0.6755537430818226\n",
      "Cost after iteration 238: 0.6755076884081201\n",
      "Cost after iteration 239: 0.6754618049504474\n",
      "Cost after iteration 240: 0.6754160920634855\n",
      "Cost after iteration 241: 0.6753705491043679\n",
      "Cost after iteration 242: 0.6753251754326711\n",
      "Cost after iteration 243: 0.6752799704104058\n",
      "Cost after iteration 244: 0.6752349334020068\n",
      "Cost after iteration 245: 0.675190063774325\n",
      "Cost after iteration 246: 0.6751453608966176\n",
      "Cost after iteration 247: 0.6751008241405394\n",
      "Cost after iteration 248: 0.6750564528801332\n",
      "Cost after iteration 249: 0.6750122464918217\n",
      "Cost after iteration 250: 0.6749682043543979\n",
      "Cost after iteration 251: 0.6749243258490162\n",
      "Cost after iteration 252: 0.6748806103591839\n",
      "Cost after iteration 253: 0.6748370572707518\n",
      "Cost after iteration 254: 0.6747936659719059\n",
      "Cost after iteration 255: 0.6747504358531582\n",
      "Cost after iteration 256: 0.674707366307338\n",
      "Cost after iteration 257: 0.6746644567295835\n",
      "Cost after iteration 258: 0.6746217065173324\n",
      "Cost after iteration 259: 0.6745791150703143\n",
      "Cost after iteration 260: 0.6745366817905406\n",
      "Cost after iteration 261: 0.6744944060822972\n",
      "Cost after iteration 262: 0.6744522873521355\n",
      "Cost after iteration 263: 0.6744103250088633\n",
      "Cost after iteration 264: 0.674368518463537\n",
      "Cost after iteration 265: 0.6743268671294529\n",
      "Cost after iteration 266: 0.6742853704221384\n",
      "Cost after iteration 267: 0.674244027759344\n",
      "Cost after iteration 268: 0.6742028385610347\n",
      "Cost after iteration 269: 0.6741618022493818\n",
      "Cost after iteration 270: 0.6741209182487541\n",
      "Cost after iteration 271: 0.6740801859857104\n",
      "Cost after iteration 272: 0.6740396048889902\n",
      "Cost after iteration 273: 0.6739991743895061\n",
      "Cost after iteration 274: 0.6739588939203357\n",
      "Cost after iteration 275: 0.6739187629167128\n",
      "Cost after iteration 276: 0.6738787808160203\n",
      "Cost after iteration 277: 0.67383894705778\n",
      "Cost after iteration 278: 0.6737992610836477\n",
      "Cost after iteration 279: 0.6737597223374016\n",
      "Cost after iteration 280: 0.6737203302649369\n",
      "Cost after iteration 281: 0.6736810843142562\n",
      "Cost after iteration 282: 0.6736419839354626\n",
      "Cost after iteration 283: 0.6736030285807513\n",
      "Cost after iteration 284: 0.6735642177044013\n",
      "Cost after iteration 285: 0.673525550762768\n",
      "Cost after iteration 286: 0.6734870272142754\n",
      "Cost after iteration 287: 0.6734486465194074\n",
      "Cost after iteration 288: 0.673410408140701\n",
      "Cost after iteration 289: 0.6733723115427381\n",
      "Cost after iteration 290: 0.6733343561921381\n",
      "Cost after iteration 291: 0.673296541557549\n",
      "Cost after iteration 292: 0.6732588671096413\n",
      "Cost after iteration 293: 0.673221332321099\n",
      "Cost after iteration 294: 0.6731839366666129\n",
      "Cost after iteration 295: 0.6731466796228723\n",
      "Cost after iteration 296: 0.6731095606685579\n",
      "Cost after iteration 297: 0.6730725792843338\n",
      "Cost after iteration 298: 0.6730357349528403\n",
      "Cost after iteration 299: 0.6729990271586864\n"
     ]
    }
   ],
   "source": [
    "layer_dims = [11,4,3,2,1]\n",
    "parameters, costs = L_layer_model(X, Y , layer_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1817,
   "id": "559f9e05-b09e-492a-88fb-fae5aa725c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_new, y_new):\n",
    "\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2\n",
    "    p = np.zeros((1, m))\n",
    "\n",
    "    probas, caches = L_model_forward(X_new, parameters)\n",
    "\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0, i] > 0.5:\n",
    "            p[0, i] = 1\n",
    "        else:\n",
    "            p[0, i] = 0\n",
    "\n",
    "    print(\"Accuracy = \" + str(np.sum(p == y_new)/m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1818,
   "id": "5390e0d8-0619-4984-b788-0ce4824fb52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9990630124150855\n"
     ]
    }
   ],
   "source": [
    "x_new = np.array([0.0, 0, 10, 4.270833333333333, 4.080267558528428, 6.666666666666666, 5.359897172236504, 11.25, 1.25, 3.8766519823788546, 4.125])\n",
    "\n",
    "predict(x_new, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
